---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Iâ€˜m a Ph.D. Candidate in the Department of Electrical and Computer Engineering at Florida International University (FIU), where I work as a Graduate Research&Teaching Assistant in Dr. Ou Baiâ€™s Human Cyber-Physical Systems (HCPS) Laboratory. My research focuses on the intersection of Computer Vision, Deep Learning, Embedded Systems, and Multimodal Large Language Models (MLLMs), with an emphasis on real-time, resource-constrained, and wearable AI systems. I have published research in venues across multimedia, biometrics, computer vision, and natural language processing at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=k--3fM4AAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

Currently, I'm working on an AI-powered wearable multimodal multi-task real-time dynamic behavior analysis system, aiming to integrate perception, reasoning, and decision-making into embedded and edge AI platforms. Particularly interested in efficient multimodal LLM inference, model compression, and edge/embedded AI deployment, with the goal of enabling intelligent systems to operate reliably under strict latency, computation, and energy constraints. My research has been applied to a range of real-world cyber-physical systems, including wearable exoskeletons, multimodal emotion recognition, and humanâ€“AI interaction systems.

ğŸ“« Contact: pxiang@fiu.edu


# ğŸ”¥ News
- *2025.08*: &nbsp;ğŸ‰ğŸ‰ The [[**MTCAE-DFER Project**](https://github.com/Peihao-Xiang/MTCAE-DFER)] code has been uploaded to GitHub.
- *2025.07*: &nbsp;ğŸ‰ğŸ‰ The paper [**MTCAE-DFER**](https://arxiv.org/abs/2412.18988) has been accepted by the 9th IEEE International Joint Conference on Biometrics!
- *2025.06*: &nbsp;ğŸ‰ğŸ‰ The paper [**Label Ranker**](https://doi.org/10.1145/3731715.3733369) has been published on ACM DL.
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ The [[**Label Ranker Project**](https://github.com/Peihao-Xiang/Label-Ranker)] code has been uploaded to GitHub.
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ The paper [**Label Ranker**](https://www.preprints.org/manuscript/202503.0003) has been accepted by the 15th ACM International Conference on Multimedia Retrieval!

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICMR 2025</div><img src='Paper/Label_Orderer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Label Ranker: Self-aware Preference for Classification Label Position in Visual Masked Self-supervised Pre-trained Model](https://doi.org/10.1145/3731715.3733369)

**Peihao Xiang**, Kaida Wu, Ou Bai

ACM 15th International Conference on Multimedia Retrieval

30 June - 3 July, 2025 \| Chicago, IL, USA

[[**ICMR 2025**](https://www.icmr-2025.org/)][[**Paper**](https://doi.org/10.1145/3731715.3733369)][[**Preprints**](https://www.preprints.org/manuscript/202503.0003)][[**Code**](https://github.com/Peihao-Xiang/Label-Ranker)][[**Video**](https://drive.google.com/file/d/1MhP51vQf-NjJPpR_13V6N2MW46kTq67b/view?usp=drive_link)]
- Label Ranker introduces a self-aware label positioning mechanism for visual masked self-supervised pre-trained models (e.g., MAE-style architectures). It observes that the position at which a classification label (or token) is injected into the model significantly affects downstream performance, yet is usually treated as a fixed or arbitrary design choice.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICPRS 2024</div><img src='Paper/MultiMAE-DER_V2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion Recognition](https://doi.org/10.1109/ICPRS62101.2024.10677820)

**Peihao Xiang**, Chaohao Lin, Kaida Wu, Ou Bai

IEEE 14th International Conference on Pattern Recognition Systems

15-18 July, 2024 \| University of Westminster, London, UK

[[**ICPRS 2024**](http://s836450039.websitehome.co.uk/icprs24/index.html#intro)][[**Paper**](https://doi.org/10.1109/ICPRS62101.2024.10677820)][[**arXiv**](https://arxiv.org/abs/2404.18327)][[**Code**](https://github.com/Peihao-Xiang/MultiMAE-DER)][[**Video**](https://www.youtube.com/watch?v=HqaTGxtQaEo)]
- MultiMAE-DER proposes a multimodal masked autoencoder framework for dynamic emotion recognition (DER) that learns robust temporalâ€“emotional representations by jointly modeling multiple modalities (e.g., visual, audio, and physiological signals). 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# ğŸ– Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ“– Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# ğŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# ğŸ’» Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
